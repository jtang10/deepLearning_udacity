{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Assignment 3\n",
    "The goal of this assignment is to explore regularization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = './data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Introduce and tune L2 regularization for both logistic and neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "        beta_regul * tf.nn.l2_loss(weights)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.668545\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 13.0%\n",
      "Minibatch loss at step 500: 2.467217\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 1000: 1.762559\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1500: 0.871309\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 2000: 0.828484\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2500: 0.844550\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.774282\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dict telling the session where to feed the batch.\n",
    "        # The key is the placeholder node to be fed and value is the ndarray\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -1, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for beta in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dict telling the session where to feed the batch.\n",
    "            # The key is the placeholder node to be fed and value is the ndarray\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEMCAYAAADJQLEhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11618e110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 645.036133\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 30.5%\n",
      "Minibatch loss at step 500: 196.086380\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1000: 115.433235\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1500: 68.792587\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2000: 41.404346\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 2500: 25.123196\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3000: 15.464598\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dict telling the session where to feed the batch.\n",
    "        # The key is the placeholder node to be fed and value is the ndarray\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Demonstrate an extreme case of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 124\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 497.716217\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 14.8%\n",
      "Minibatch loss at step 10: 353.135498\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 20: 349.621216\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 30: 346.141449\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 40: 342.696014\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 50: 339.285004\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 60: 335.908691\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 70: 332.565399\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 80: 329.255951\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 90: 325.978790\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 100: 322.734436\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 19.5%\n",
      "Test accuracy: 20.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 4\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = step % num_batches\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dict telling the session where to feed the batch.\n",
    "        # The key is the placeholder node to be fed and value is the ndarray\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Introduce Dropout on the hidden layer of the neural network. Dropout should only be used during the training, not the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 124\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 785.952271\n",
      "Minibatch accuracy: 11.3%\n",
      "Validation accuracy: 18.7%\n",
      "Minibatch loss at step 10: 323.374817\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 20: 316.026917\n",
      "Minibatch accuracy: 96.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 30: 306.416382\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 40: 303.824890\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 50: 300.778748\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 60: 297.942322\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 70: 294.231812\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.6%\n",
      "Minibatch loss at step 80: 291.557953\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 90: 288.432465\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 100: 285.562531\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Test accuracy: 73.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 4\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = step % num_batches\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dict telling the session where to feed the batch.\n",
    "        # The key is the placeholder node to be fed and value is the ndarray\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
